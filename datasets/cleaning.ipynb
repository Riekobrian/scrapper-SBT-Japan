{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing dataset: C:\\Users\\Ricky\\Desktop\\For Fun Projects\\SBT-Japan\\datasets\\merged_used_cars_cleaned.csv\n",
      "\n",
      "=== Basic Dataset Information ===\n",
      "Shape: (13467, 15)\n",
      "Columns: ['Make', 'Model', 'Year', 'Mileage', 'Engine size', 'Fuel type', 'Transmission', 'Condition', 'Drive type', 'Horsepower', 'Torque', 'Acceleration', 'Body type', 'Seats', 'Price']\n",
      "Total Rows: 13467\n",
      "Memory Usage: 1.54 MB\n",
      "\n",
      "=== Missing Values Summary ===\n",
      "              Missing Values  Missing (%)\n",
      "Acceleration           10687        79.36\n",
      "Horsepower             10680        79.30\n",
      "Torque                 10678        79.29\n",
      "Condition               9653        71.68\n",
      "Seats                   4872        36.18\n",
      "Year                    2414        17.93\n",
      "Drive type               946         7.02\n",
      "Body type                728         5.41\n",
      "Price                    622         4.62\n",
      "Mileage                  551         4.09\n",
      "Engine size              123         0.91\n",
      "Transmission             121         0.90\n",
      "Fuel type                 15         0.11\n",
      "Make                       0         0.00\n",
      "Model                      0         0.00\n",
      "\n",
      "Rows with at least one missing value: 13467 (100.00%)\n",
      "\n",
      "=== Column Data Types ===\n",
      "Make             object\n",
      "Model            object\n",
      "Year             object\n",
      "Mileage          object\n",
      "Engine size      object\n",
      "Fuel type        object\n",
      "Transmission     object\n",
      "Condition        object\n",
      "Drive type       object\n",
      "Horsepower      float64\n",
      "Torque          float64\n",
      "Acceleration    float64\n",
      "Body type        object\n",
      "Seats            object\n",
      "Price            object\n",
      "dtype: object\n",
      "\n",
      "=== Detailed Column Analysis ===\n",
      "\n",
      "--- Analyzing Column: 'Make' ---\n",
      "Total Rows: 13467\n",
      "Missing Values: 0 (0.00%)\n",
      "Unique Values: 58\n",
      "Data Type: object\n",
      "Sample of Unique Values (10 shown):\n",
      "['Toyota' 'Mercedes' 'Mazda' 'Subaru' 'Bmw' 'Volkswagen' 'Nissan' 'Land'\n",
      " 'Porsche' 'Lexus']\n",
      "\n",
      "--- Analyzing Column: 'Model' ---\n",
      "Total Rows: 13467\n",
      "Missing Values: 0 (0.00%)\n",
      "Unique Values: 1119\n",
      "Data Type: object\n",
      "Sample of Unique Values (10 shown):\n",
      "['Land Cruiser' 'Crown' 'B' 'Cla' 'A' 'Fielder' 'Land Cruiser Prado'\n",
      " 'Vitz' 'Cx8' 'Outback']\n",
      "Inconsistency Detected: Numeric values with text suffixes/prefixes.\n",
      "Examples:\n",
      "['550I', 'Landcruiser 300 Series', 'Rx450H', 'Cx5 2018', 'Vezel 2018']\n",
      "\n",
      "--- Analyzing Column: 'Year' ---\n",
      "Total Rows: 13467\n",
      "Missing Values: 2414 (17.93%)\n",
      "Unique Values: 421\n",
      "Data Type: object\n",
      "Sample of Unique Values (10 shown):\n",
      "['2021' '2024' '2023' '2018' '2017' '2020' '2022' '2019' '2016' '2015']\n",
      "\n",
      "--- Analyzing Column: 'Mileage' ---\n",
      "Total Rows: 13467\n",
      "Missing Values: 551 (4.09%)\n",
      "Unique Values: 2515\n",
      "Data Type: object\n",
      "Sample of Unique Values (10 shown):\n",
      "['10944 Km' '9,000kms Km' '3,000KMS Km' '93150 Km' '31222 Km' '58510 Km'\n",
      " '28172 Km' '54700 Km' '30600 Km' '11670 Km']\n",
      "Inconsistency Detected: Special characters (e.g., commas) present.\n",
      "Examples with special characters:\n",
      "['174,000km', '4,000km', '144,000km', '161,000km', '1,725km']\n",
      "Inconsistency Detected: Numeric values with text suffixes/prefixes.\n",
      "Examples:\n",
      "['146,000km', '133,000km', '167,000km', '202,000km', '154,000km']\n",
      "\n",
      "--- Analyzing Column: 'Engine size' ---\n",
      "Total Rows: 13467\n",
      "Missing Values: 123 (0.91%)\n",
      "Unique Values: 383\n",
      "Data Type: object\n",
      "Sample of Unique Values (10 shown):\n",
      "['3300 cc' '2500 cc' '1600 cc' '1800 cc' '3000 cc' '1000 cc' '1500 cc'\n",
      " '2200 cc' '2800 cc' '4700 cc']\n",
      "Inconsistency Detected: Variations in casing or spacing found.\n",
      "Inconsistency Detected: Special characters (e.g., commas) present.\n",
      "Examples with special characters:\n",
      "['1,990cc', '1,490cc', '2,000cc', '4,000cc', '1,490cc']\n",
      "Inconsistency Detected: Numeric values with text suffixes/prefixes.\n",
      "Examples:\n",
      "['1,490cc', '1,200cc', '1,990cc', '2,500cc', '4,000cc']\n",
      "\n",
      "--- Analyzing Column: 'Fuel type' ---\n",
      "Total Rows: 13467\n",
      "Missing Values: 15 (0.11%)\n",
      "Unique Values: 13\n",
      "Data Type: object\n",
      "Sample of Unique Values (10 shown):\n",
      "['Diesel' 'Petrol' 'Petrol Hybrid' 'Hybrid' 'Petroleum' 'Diesel Hybrid'\n",
      " 'Electric' 'PETROL' 'HYBRID(PETROL)' 'DIESEL']\n",
      "Inconsistency Detected: Variations in casing or spacing found.\n",
      "\n",
      "--- Analyzing Column: 'Transmission' ---\n",
      "Total Rows: 13467\n",
      "Missing Values: 121 (0.90%)\n",
      "Unique Values: 11\n",
      "Data Type: object\n",
      "Sample of Unique Values (10 shown):\n",
      "['Automatic' 'Manual' 'AT' '6MT' 'MT' 'DUONIC' '7MT' '5MT' 'SMOOTHER'\n",
      " 'PROSHIFT']\n",
      "Inconsistency Detected: Numeric values with text suffixes/prefixes.\n",
      "Examples:\n",
      "['6MT', '5MT', '5MT', '5MT', '6MT']\n",
      "\n",
      "--- Analyzing Column: 'Condition' ---\n",
      "Total Rows: 13467\n",
      "Missing Values: 9653 (71.68%)\n",
      "Unique Values: 15\n",
      "Data Type: object\n",
      "Sample of Unique Values (10 shown):\n",
      "['4.5' '4' '5' '6' '5, 4.5' 'Unsold' 'Sold' 'Foreign used' 'Locally used'\n",
      " 'New']\n",
      "Inconsistency Detected: Special characters (e.g., commas) present.\n",
      "Examples with special characters:\n",
      "['5, 4.5']\n",
      "Inconsistency Detected: Numeric values with text suffixes/prefixes.\n",
      "Examples:\n",
      "[]\n",
      "\n",
      "--- Analyzing Column: 'Drive type' ---\n",
      "Total Rows: 13467\n",
      "Missing Values: 946 (7.02%)\n",
      "Unique Values: 10\n",
      "Data Type: object\n",
      "Sample of Unique Values (10 shown):\n",
      "['AWD' '2WD' '4WD' 'FWD' 'RWD' 'MR' 'FR' 'RR' 'FF' '04-Feb']\n",
      "Inconsistency Detected: Numeric values with text suffixes/prefixes.\n",
      "Examples:\n",
      "['2WD', '2WD', '2WD', '4WD', '2WD']\n",
      "\n",
      "--- Analyzing Column: 'Horsepower' ---\n",
      "Total Rows: 13467\n",
      "Missing Values: 10680 (79.30%)\n",
      "Unique Values: 286\n",
      "Data Type: float64\n",
      "Sample of Unique Values (10 shown):\n",
      "[203. 170. 222. 201. 237. 109. 150. 148. 220. 197.]\n",
      "Min: 46.0\n",
      "Max: 1841.0\n",
      "Mean: 209.9508432005741\n",
      "Median: 178.0\n",
      "Potential Outliers: 160 (5.74%)\n",
      "\n",
      "--- Analyzing Column: 'Torque' ---\n",
      "Total Rows: 13467\n",
      "Missing Values: 10678 (79.29%)\n",
      "Unique Values: 261\n",
      "Data Type: float64\n",
      "Sample of Unique Values (10 shown):\n",
      "[250. 231. 258. 500. 360. 141. 192. 400. 221. 280.]\n",
      "Min: 50.0\n",
      "Max: 900.0\n",
      "Mean: 329.2757260666906\n",
      "Median: 300.0\n",
      "Potential Outliers: 51 (1.83%)\n",
      "\n",
      "--- Analyzing Column: 'Acceleration' ---\n",
      "Total Rows: 13467\n",
      "Missing Values: 10687 (79.36%)\n",
      "Unique Values: 105\n",
      "Data Type: float64\n",
      "Sample of Unique Values (10 shown):\n",
      "[ 9.  10.   8.  11.7 11.   6.   8.1  5.1 10.5  7.8]\n",
      "Min: 2.8\n",
      "Max: 450.0\n",
      "Mean: 8.646258992805755\n",
      "Median: 8.0\n",
      "Potential Outliers: 43 (1.55%)\n",
      "\n",
      "--- Analyzing Column: 'Body type' ---\n",
      "Total Rows: 13467\n",
      "Missing Values: 728 (5.41%)\n",
      "Unique Values: 23\n",
      "Data Type: object\n",
      "Sample of Unique Values (10 shown):\n",
      "['SUV' 'Saloon' 'Mini SUV' 'Coupes' 'Hatchback' 'Wagon' 'Minivan' 'Pickup'\n",
      " 'Buses and Vans' 'Van']\n",
      "\n",
      "--- Analyzing Column: 'Seats' ---\n",
      "Total Rows: 13467\n",
      "Missing Values: 4872 (36.18%)\n",
      "Unique Values: 25\n",
      "Data Type: object\n",
      "Sample of Unique Values (10 shown):\n",
      "['7' '5' '6' '8' '2[5]' '4' '2' '2[4]' '10' '29']\n",
      "\n",
      "--- Analyzing Column: 'Price' ---\n",
      "Total Rows: 13467\n",
      "Missing Values: 622 (4.62%)\n",
      "Unique Values: 6633\n",
      "Data Type: object\n",
      "Sample of Unique Values (10 shown):\n",
      "['KSh1,300,000' 'KSh6,899,999' 'KSh3,650,000' 'KSh2,400,000'\n",
      " 'KSh9,800,000' 'KSh6,800,000' 'KSh3,999,999' 'KSh4,200,000'\n",
      " 'KSh6,300,000' 'KSh2']\n",
      "Inconsistency Detected: Special characters (e.g., commas) present.\n",
      "Examples with special characters:\n",
      "['KSh1,749,999', 'KSh6,500,000', 'KSh6,899,999', 'KSh1,799,999', 'KSh2,849,999']\n",
      "Inconsistency Detected: Numeric values with text suffixes/prefixes.\n",
      "Examples:\n",
      "['KSh4,650,000', 'KSh2,629,999', 'KSh4,600,000', 'KSh2,300,000', 'KSh4,200,000']\n",
      "\n",
      "=== Specific Data Quality Issues ===\n",
      "\n",
      "Condition column values:\n",
      "['4.5' '4' '5' '6' '5, 4.5' 'Unsold' 'Sold' 'Foreign used' 'Locally used'\n",
      " 'New' 'Ready for Import' 'Very Good' 'Excellent' 'Average'\n",
      " 'Below Average']\n",
      "Issue Detected: Condition column has mixed numeric and non-numeric values\n",
      "Numeric values: ['4.5', '4', '5', '6', '5, 4.5']\n",
      "Non-numeric values: ['Unsold', 'Sold', 'Foreign used', 'Locally used', 'New']\n",
      "\n",
      "Make-Model frequency distribution:\n",
      "Total unique Make-Model combinations: 1185\n",
      "Top 10 Make-Model combinations:\n",
      "        Make         Model  count\n",
      "1134  Toyota          Vitz    321\n",
      "228    Honda           Fit    319\n",
      "1009  Toyota  Land Cruiser    309\n",
      "1079  Toyota        Ractis    276\n",
      "721   Nissan          Note    228\n",
      "418    Mazda         Demio    191\n",
      "402    Mazda           Cx5    189\n",
      "767   Nissan        Xtrail    178\n",
      "820   Subaru      Forester    176\n",
      "1144  Toyota          Wish    160\n",
      "\n",
      "Rare Make-Model combinations (fewer than 5 entries): 701\n",
      "\n",
      "Potential Data Quality Issue: Same model name appearing under multiple makes\n",
      "Model 'Rover Vogue' appears under makes: ['Range', 'Ranger']\n",
      "Model 'Nan' appears under makes: ['Mercedes', 'Peugeot', 'Peugot', 'Wish', 'Volks', 'Nan']\n",
      "Model 'Ranger' appears under makes: ['Ford', 'Hino']\n",
      "Model 'Continental' appears under makes: ['Bently', 'Bentley', 'Mazda', 'Nissan', 'Daihatsu', 'Toyota']\n",
      "Model 'Lexus Lx' appears under makes: ['Toyota', 'Nan']\n",
      "\n",
      "--- Specific Check: Rows 728–730 for 'Make' and 'Model' ---\n",
      "    Make        Model\n",
      "728  Nan  Toyota Rav4\n",
      "729  Nan  Toyota Axio\n",
      "730  Nan  Toyota Rav4\n",
      "\n",
      "Columns with 'Nan' string values (not actual NaN):\n",
      "- Make: 329 rows\n",
      "- Model: 8 rows\n",
      "\n",
      "=== Overall Dataset Diversity ===\n",
      "Total Rows: 13467\n",
      "Total Columns: 15\n",
      "Rows with at least one missing value: 13467\n",
      "Percentage of rows with missing values: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Perform comprehensive analysis on a dataset to identify inconsistencies,\n",
    "    data quality issues, and structural problems.\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing dataset: {file_path}\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Basic dataset information\n",
    "    print(f\"\\n=== Basic Dataset Information ===\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Total Rows: {len(df)}\")\n",
    "    print(f\"Memory Usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Missing values summary\n",
    "    missing_values = df.isna().sum()\n",
    "    missing_percent = (missing_values / len(df)) * 100\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Missing Values': missing_values,\n",
    "        'Missing (%)': missing_percent.round(2)\n",
    "    }).sort_values('Missing Values', ascending=False)\n",
    "    \n",
    "    print(f\"\\n=== Missing Values Summary ===\")\n",
    "    print(missing_summary)\n",
    "    \n",
    "    # Rows with missing values\n",
    "    rows_with_missing = df.isna().any(axis=1).sum()\n",
    "    print(f\"\\nRows with at least one missing value: {rows_with_missing} ({rows_with_missing/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Column type analysis\n",
    "    print(f\"\\n=== Column Data Types ===\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Detailed column analysis\n",
    "    print(f\"\\n=== Detailed Column Analysis ===\")\n",
    "    \n",
    "    for col in df.columns:\n",
    "        print(f\"\\n--- Analyzing Column: '{col}' ---\")\n",
    "        print(f\"Total Rows: {len(df)}\")\n",
    "        print(f\"Missing Values: {df[col].isna().sum()} ({df[col].isna().sum()/len(df)*100:.2f}%)\")\n",
    "        \n",
    "        # Handle different data types appropriately\n",
    "        if df[col].dtype == 'object':\n",
    "            # For string/object columns\n",
    "            non_missing_values = df[col].dropna()\n",
    "            unique_values = non_missing_values.nunique()\n",
    "            print(f\"Unique Values: {unique_values}\")\n",
    "            print(f\"Data Type: {df[col].dtype}\")\n",
    "            \n",
    "            # Show sample of unique values\n",
    "            if unique_values > 0:\n",
    "                sample_size = min(10, unique_values)\n",
    "                print(f\"Sample of Unique Values ({sample_size} shown):\")\n",
    "                print(non_missing_values.unique()[:sample_size])\n",
    "            \n",
    "            # Check for inconsistencies in formatting\n",
    "            if unique_values > 0:\n",
    "                # Check for variations in casing or leading/trailing spaces\n",
    "                values_lower = non_missing_values.str.lower().str.strip()\n",
    "                if values_lower.nunique() < unique_values:\n",
    "                    print(\"Inconsistency Detected: Variations in casing or spacing found.\")\n",
    "                \n",
    "                # Check for special characters (commas, etc.)\n",
    "                has_special_chars = non_missing_values.str.contains(r'[,]').any()\n",
    "                if has_special_chars:\n",
    "                    print(\"Inconsistency Detected: Special characters (e.g., commas) present.\")\n",
    "                    examples = non_missing_values[non_missing_values.str.contains(r'[,]')].sample(min(5, non_missing_values.str.contains(r'[,]').sum())).tolist()\n",
    "                    print(f\"Examples with special characters:\")\n",
    "                    print(examples)\n",
    "                \n",
    "                # Check for numeric values stored as strings\n",
    "                looks_numeric = non_missing_values.str.replace(r'[^0-9.]', '', regex=True).str.match(r'^[0-9]+(\\.[0-9]+)?$').any()\n",
    "                if looks_numeric:\n",
    "                    contains_text = non_missing_values.str.contains(r'[a-zA-Z]').any()\n",
    "                    if contains_text:\n",
    "                        print(\"Inconsistency Detected: Numeric values with text suffixes/prefixes.\")\n",
    "                        examples = non_missing_values[non_missing_values.str.contains(r'[0-9]') & non_missing_values.str.contains(r'[a-zA-Z]')].sample(min(5, (non_missing_values.str.contains(r'[0-9]') & non_missing_values.str.contains(r'[a-zA-Z]')).sum())).tolist()\n",
    "                        print(f\"Examples:\")\n",
    "                        print(examples)\n",
    "        else:\n",
    "            # For numeric columns\n",
    "            non_missing_values = df[col].dropna()\n",
    "            unique_values = non_missing_values.nunique()\n",
    "            print(f\"Unique Values: {unique_values}\")\n",
    "            print(f\"Data Type: {df[col].dtype}\")\n",
    "            \n",
    "            if len(non_missing_values) > 0:\n",
    "                # Show sample of unique values\n",
    "                sample_size = min(10, unique_values)\n",
    "                if sample_size > 0:\n",
    "                    print(f\"Sample of Unique Values ({sample_size} shown):\")\n",
    "                    print(non_missing_values.unique()[:sample_size])\n",
    "                \n",
    "                # Basic statistics\n",
    "                print(f\"Min: {non_missing_values.min()}\")\n",
    "                print(f\"Max: {non_missing_values.max()}\")\n",
    "                print(f\"Mean: {non_missing_values.mean()}\")\n",
    "                print(f\"Median: {non_missing_values.median()}\")\n",
    "                \n",
    "                # Check for outliers (using IQR method)\n",
    "                q1 = non_missing_values.quantile(0.25)\n",
    "                q3 = non_missing_values.quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                outliers = ((non_missing_values < (q1 - 1.5 * iqr)) | (non_missing_values > (q3 + 1.5 * iqr))).sum()\n",
    "                if outliers > 0:\n",
    "                    print(f\"Potential Outliers: {outliers} ({outliers/len(non_missing_values)*100:.2f}%)\")\n",
    "    \n",
    "    # Check for specific problem areas based on errors\n",
    "    print(f\"\\n=== Specific Data Quality Issues ===\")\n",
    "    \n",
    "    # Check condition column for mixed types\n",
    "    if 'Condition' in df.columns:\n",
    "        condition_values = df['Condition'].dropna().unique()\n",
    "        print(\"\\nCondition column values:\")\n",
    "        print(condition_values)\n",
    "        \n",
    "        # Try to detect if values are numeric or mixed\n",
    "        numeric_pattern = re.compile(r'^[0-9]+(\\.[0-9]+)?$')\n",
    "        numeric_values = [v for v in condition_values if isinstance(v, (int, float)) or \n",
    "                         (isinstance(v, str) and numeric_pattern.match(re.sub(r'[^0-9.]', '', v)))]\n",
    "        non_numeric_values = [v for v in condition_values if v not in numeric_values]\n",
    "        \n",
    "        if numeric_values and non_numeric_values:\n",
    "            print(\"Issue Detected: Condition column has mixed numeric and non-numeric values\")\n",
    "            print(f\"Numeric values: {numeric_values[:5]}\")\n",
    "            print(f\"Non-numeric values: {non_numeric_values[:5]}\")\n",
    "    \n",
    "    # Check the Make-Model relationship in the dataset\n",
    "    if 'Make' in df.columns and 'Model' in df.columns:\n",
    "        make_model_counts = df.groupby(['Make', 'Model']).size().reset_index(name='count')\n",
    "        make_model_counts = make_model_counts.sort_values('count', ascending=False)\n",
    "        \n",
    "        print(\"\\nMake-Model frequency distribution:\")\n",
    "        print(f\"Total unique Make-Model combinations: {len(make_model_counts)}\")\n",
    "        print(f\"Top 10 Make-Model combinations:\")\n",
    "        print(make_model_counts.head(10))\n",
    "        \n",
    "        print(f\"\\nRare Make-Model combinations (fewer than 5 entries): {len(make_model_counts[make_model_counts['count'] < 5])}\")\n",
    "        \n",
    "        # Check for potential model misclassifications\n",
    "        if len(make_model_counts) > 0:\n",
    "            model_by_make = {}\n",
    "            for make in df['Make'].dropna().unique():\n",
    "                models = df[df['Make'] == make]['Model'].dropna().unique()\n",
    "                model_by_make[make] = models\n",
    "            \n",
    "            # Look for models appearing under multiple makes\n",
    "            all_models = df['Model'].dropna().unique()\n",
    "            shared_models = []\n",
    "            for model in all_models:\n",
    "                makes_with_model = df[df['Model'] == model]['Make'].dropna().unique()\n",
    "                if len(makes_with_model) > 1:\n",
    "                    shared_models.append((model, list(makes_with_model)))\n",
    "            \n",
    "            if shared_models:\n",
    "                print(\"\\nPotential Data Quality Issue: Same model name appearing under multiple makes\")\n",
    "                for model, makes in shared_models[:5]:\n",
    "                    print(f\"Model '{model}' appears under makes: {makes}\")\n",
    "    \n",
    "    # Check rows 728-730 specifically (as mentioned in error)\n",
    "    if len(df) > 730:\n",
    "        print(\"\\n--- Specific Check: Rows 728–730 for 'Make' and 'Model' ---\")\n",
    "        specific_rows = df.iloc[728:731]\n",
    "        print(specific_rows[['Make', 'Model']])\n",
    "    \n",
    "    # Check for 'Nan' strings vs actual NaN values\n",
    "    nan_strings = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            nan_string_count = (df[col] == 'Nan').sum()\n",
    "            if nan_string_count > 0:\n",
    "                nan_strings.append((col, nan_string_count))\n",
    "    \n",
    "    if nan_strings:\n",
    "        print(\"\\nColumns with 'Nan' string values (not actual NaN):\")\n",
    "        for col, count in nan_strings:\n",
    "            print(f\"- {col}: {count} rows\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Update this with your file path\n",
    "    file_path = r\"C:\\Users\\Ricky\\Desktop\\For Fun Projects\\SBT-Japan\\datasets\\merged_used_cars_cleaned.csv\"\n",
    "    \n",
    "    # Analyze the dataset\n",
    "    df = analyze_dataset(file_path)\n",
    "    \n",
    "    print(\"\\n=== Overall Dataset Diversity ===\")\n",
    "    print(f\"Total Rows: {len(df)}\")\n",
    "    print(f\"Total Columns: {len(df.columns)}\")\n",
    "    rows_with_missing = df.isna().any(axis=1).sum()\n",
    "    print(f\"Rows with at least one missing value: {rows_with_missing}\")\n",
    "    print(f\"Percentage of rows with missing values: {rows_with_missing/len(df)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: C:\\Users\\Ricky\\Desktop\\For Fun Projects\\SBT-Japan\\datasets\\merged_used_cars_cleaned.csv\n",
      "Original dataset shape: (13467, 15)\n",
      "\n",
      "=== Cleaning Make and Model ===\n",
      "Found 329 rows with 'Nan' string in Make column\n",
      "Extracting Make from Model for 329 rows\n",
      "\n",
      "=== Cleaning Numerical and Categorical Columns ===\n",
      "\n",
      "=== Checking for Outliers ===\n",
      "Found 222 outliers in Engine size (1.66%)\n",
      "  Range: [0.0, 60006.0], IQR bounds: [-2740.0, 7130.0]\n",
      "  After capping: Range: [500.0, 9830.0]\n",
      "Found 116 outliers in Mileage (0.90%)\n",
      "  Range: [0.0, 1375000.0], IQR bounds: [-230000.0, 414000.0]\n",
      "  After capping: Range: [0.0, 1375000.0]\n",
      "Found 2139 outliers in Price (16.75%)\n",
      "  Range: [2.0, 69783000.0], IQR bounds: [-1555622.5, 2084181.5]\n",
      "  After capping: Range: [10002.0, 69783000.0]\n",
      "Found 43 outliers in Horsepower (1.54%)\n",
      "  Range: [46.0, 1841.0], IQR bounds: [-192.0, 585.0]\n",
      "  After capping: Range: [46.0, 1841.0]\n",
      "Found 5 outliers in Acceleration (0.18%)\n",
      "  Range: [2.8, 450.0], IQR bounds: [-2.0, 19.0]\n",
      "  After capping: Range: [2.8, 30.0]\n",
      "\n",
      "=== Imputing Missing Values ===\n",
      "Found 1092 instances of rare models (fewer than 5 entries)\n",
      "Imputing Fuel type...\n",
      "  - Remaining missing in Fuel type: 0\n",
      "Imputing Transmission...\n",
      "  - Remaining missing in Transmission: 0\n",
      "Imputing Drive type...\n",
      "  - Remaining missing in Drive type: 0\n",
      "Imputing Body type...\n",
      "  - Remaining missing in Body type: 0\n",
      "Imputing Seats...\n",
      "  - Remaining missing in Seats: 0\n",
      "Imputing Engine size...\n",
      "  - Remaining missing in Engine size: 0\n",
      "Imputing Horsepower...\n",
      "  - Remaining missing in Horsepower: 0\n",
      "Imputing Torque...\n",
      "  - Remaining missing in Torque: 0\n",
      "Imputing Acceleration...\n",
      "  - Remaining missing in Acceleration: 0\n",
      "Imputing Year...\n",
      "  - Remaining missing in Year: 0\n",
      "Imputing Condition...\n",
      "  - Remaining missing in Condition: 0\n",
      "\n",
      "=== Running Validation Checks ===\n",
      "WARNING: Found diesel engines smaller than 1500cc (minimum: 650.0)\n",
      "  Found 59 suspicious diesel engines < 1500cc\n",
      "✅ Found 12474 cars with automatic transmission\n",
      "Price range: 10,002 to 69,783,000, Mean: 2,303,880\n",
      "WARNING: Found 1102 vehicles with suspiciously low prices\n",
      "WARNING: Still have 2 rows with missing Make\n",
      "\n",
      "=== Data Summary After Cleaning ===\n",
      "Original rows: 13467\n",
      "Final rows: 13467\n",
      "\n",
      "Missing values remaining:\n",
      "  - Make: 2 (0.0%)\n",
      "  - Mileage: 552 (4.1%)\n",
      "  - Price: 6910 (51.3%)\n",
      "  - Condition_Original: 9653 (71.7%)\n",
      "\n",
      "Saving cleaned dataset to: C:\\Users\\Ricky\\Desktop\\For Fun Projects\\SBT-Japan\\datasets\\merged_used_cars_cleaned_fully_cleaned.csv\n",
      "\n",
      "=== Sample of Cleaned Data ===\n",
      "       Make         Model    Year  Mileage  Engine size Fuel type  \\\n",
      "0    Toyota  Land Cruiser  2021.0  10944.0       3300.0    diesel   \n",
      "1    Toyota         Crown  2024.0   9000.0       2500.0    petrol   \n",
      "2    Toyota         Crown  2023.0   3000.0       2500.0    petrol   \n",
      "3  Mercedes             B  2018.0  93150.0       1600.0    petrol   \n",
      "4  Mercedes           Cla  2018.0  93150.0       1800.0    petrol   \n",
      "\n",
      "  Transmission  Condition Drive type  Horsepower  Torque  Acceleration  \\\n",
      "0    automatic        4.5        awd       150.0   200.0         10.00   \n",
      "1    automatic        4.5        fwd       220.0   377.0          8.50   \n",
      "2    automatic        4.5        fwd       211.0   250.0          8.00   \n",
      "3    automatic        4.5        fwd       208.0   350.0          7.70   \n",
      "4    automatic        4.5        fwd       122.0   230.0          8.95   \n",
      "\n",
      "       Body type  Seats  Price Condition_Original  \n",
      "0            suv    7.0    NaN                NaN  \n",
      "1  station wagon    5.0    NaN                4.5  \n",
      "2          sedan    5.0    NaN                4.5  \n",
      "3          sedan    5.0    NaN                4.5  \n",
      "4          sedan    5.0    NaN                NaN  \n",
      "\n",
      "Check rows 728-730 after cleaning:\n",
      "       Make Model\n",
      "728  Toyota  Rav4\n",
      "729  Toyota  Axio\n",
      "730  Toyota  Rav4\n",
      "\n",
      "Cleaning complete! Dataset saved to: C:\\Users\\Ricky\\Desktop\\For Fun Projects\\SBT-Japan\\datasets\\merged_used_cars_cleaned_fully_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "\n",
    "def clean_used_cars_dataset(file_path, output_suffix=\"_fully_cleaned\"):\n",
    "    \"\"\"\n",
    "    Comprehensive cleaning script for used cars dataset with improved handling for:\n",
    "    - Mixed data types in the Condition column\n",
    "    - Make extraction from Model when Make is 'Nan'\n",
    "    - Standardization of categorical columns\n",
    "    - Numeric cleaning with better error handling\n",
    "    - Careful imputation to avoid warnings\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from: {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    original_shape = df.shape\n",
    "    print(f\"Original dataset shape: {original_shape}\")\n",
    "    \n",
    "    # Generate output path in the same directory\n",
    "    file_dir = os.path.dirname(file_path)\n",
    "    file_name = os.path.basename(file_path)\n",
    "    base_name, ext = os.path.splitext(file_name)\n",
    "    output_path = os.path.join(file_dir, f\"{base_name}{output_suffix}{ext}\")\n",
    "    \n",
    "    # =================== STEP 1: CLEAN MAKE AND MODEL ===================\n",
    "    print(\"\\n=== Cleaning Make and Model ===\")\n",
    "    \n",
    "    # Replace string 'Nan' with np.nan in 'Make'\n",
    "    nan_make_count = (df['Make'] == 'Nan').sum()\n",
    "    print(f\"Found {nan_make_count} rows with 'Nan' string in Make column\")\n",
    "    df['Make'] = df['Make'].replace('Nan', np.nan)\n",
    "    \n",
    "    # Create canonical list of known makes from non-missing rows\n",
    "    known_makes = [\n",
    "        \"Toyota\", \"Mercedes\", \"BMW\", \"Land Rover\", \"Range Rover\", \"Alfa Romeo\", \"Nissan\", \"Honda\",\n",
    "        \"Porsche\", \"Audi\", \"Lexus\", \"Maserati\", \"Jaguar\", \"Bentley\", \"Ferrari\", \"AMG\", \"Volvo\",\n",
    "        \"Isuzu\", \"Mitsubishi\", \"Suzuki\", \"Subaru\", \"Volkswagen\", \"Hino\", \"Ford\", \"Lotus\", \"Aston Martin\",\n",
    "        \"Hyundai\", \"Lamborghini\", \"Mazda\", \"Peugeot\", \"Volks\", \"Cadillac\", \"Chevrolet\", \"Daihatsu\",\n",
    "        \"Kia\", \"Mini\", \"Renault\", \"Rover\", \"Jeep\", \"Citroen\", \"Datsun\", \"Fiat\", \"Leyland\", \"Tesla\"\n",
    "    ]\n",
    "    \n",
    "    # Standardize known makes for matching (proper casing)\n",
    "    known_makes = [make.title() for make in known_makes]\n",
    "    \n",
    "    # Function to extract Make from Model with improved handling\n",
    "    def extract_make_from_model(model):\n",
    "        if pd.isna(model) or not isinstance(model, str) or model.strip() == '':\n",
    "            return np.nan, model\n",
    "        \n",
    "        # Clean model string\n",
    "        model = str(model).strip()\n",
    "        model_lower = model.lower()\n",
    "        \n",
    "        # Specific handling for 'Toyota Model' pattern found in rows 728-730\n",
    "        if model_lower.startswith('toyota '):\n",
    "            return 'Toyota', model[7:].strip()\n",
    "            \n",
    "        # For other patterns, check against known makes list\n",
    "        for make in known_makes:\n",
    "            make_lower = make.lower()\n",
    "            if model_lower.startswith(make_lower + ' '):\n",
    "                return make, model[len(make) + 1:].strip()\n",
    "            elif model_lower == make_lower:\n",
    "                return make, \"\"\n",
    "        \n",
    "        # Handle special cases like 'Land Cruiser' -> 'Toyota'\n",
    "        if 'land cruiser' in model_lower and not model_lower.startswith('land rover'):\n",
    "            return 'Toyota', model\n",
    "            \n",
    "        # Failed to extract a make\n",
    "        return np.nan, model\n",
    "    \n",
    "    # Apply extraction where 'Make' is NaN\n",
    "    make_extraction_count = df['Make'].isna().sum()\n",
    "    print(f\"Extracting Make from Model for {make_extraction_count} rows\")\n",
    "    \n",
    "    # Create a mask for rows that need Make extraction\n",
    "    make_extraction_mask = df['Make'].isna()\n",
    "    \n",
    "    if make_extraction_mask.sum() > 0:\n",
    "        # Apply the extraction function\n",
    "        extracted_data = df.loc[make_extraction_mask, 'Model'].apply(extract_make_from_model)\n",
    "        \n",
    "        # Split the result into Make and updated Model\n",
    "        extracted_makes, updated_models = zip(*extracted_data)\n",
    "        \n",
    "        # Update the DataFrame\n",
    "        df.loc[make_extraction_mask, 'Make'] = extracted_makes\n",
    "        df.loc[make_extraction_mask, 'Model'] = updated_models\n",
    "    \n",
    "    # =================== STEP 2: CLEAN AND STANDARDIZE COLUMNS ===================\n",
    "    print(\"\\n=== Cleaning Numerical and Categorical Columns ===\")\n",
    "    \n",
    "    # Clean Year column to ensure it's numeric\n",
    "    if 'Year' in df.columns:\n",
    "        df['Year'] = pd.to_numeric(df['Year'].str.extract(r'(\\d{4})')[0], errors='coerce')\n",
    "    \n",
    "    # Clean Engine size column - handle both cc and L formats\n",
    "    def clean_engine_size(value):\n",
    "        if pd.isna(value) or (isinstance(value, str) and value.strip() == ''):\n",
    "            return np.nan\n",
    "        \n",
    "        try:\n",
    "            value_str = str(value).lower().strip()\n",
    "            # Remove all non-numeric characters except decimal point\n",
    "            # But first check if it's in liters format\n",
    "            is_liters = 'l' in value_str and not 'cc' in value_str\n",
    "            \n",
    "            # Remove all commas, spaces and letters\n",
    "            cleaned_value = re.sub(r'[^\\d.]', '', value_str)\n",
    "            if not cleaned_value:\n",
    "                return np.nan\n",
    "                \n",
    "            numeric_value = float(cleaned_value)\n",
    "            \n",
    "            # Convert liters to cc if necessary\n",
    "            if is_liters and numeric_value < 10:  # Assume values < 10 are in liters\n",
    "                return numeric_value * 1000\n",
    "            return numeric_value\n",
    "        except (ValueError, TypeError) as e:\n",
    "            print(f\"Error cleaning engine size '{value}': {e}\")\n",
    "            return np.nan\n",
    "    \n",
    "    df['Engine size'] = df['Engine size'].apply(clean_engine_size)\n",
    "    \n",
    "    # Clean Mileage column - standardize to kilometers without text\n",
    "    def clean_mileage(value):\n",
    "        if pd.isna(value) or (isinstance(value, str) and value.strip() == ''):\n",
    "            return np.nan\n",
    "        \n",
    "        try:\n",
    "            value_str = str(value).lower().strip()\n",
    "            # Remove all commas, spaces and letters\n",
    "            cleaned_value = re.sub(r'[^\\d.]', '', value_str)\n",
    "            if not cleaned_value:\n",
    "                return np.nan\n",
    "            return float(cleaned_value)\n",
    "        except (ValueError, TypeError) as e:\n",
    "            print(f\"Error cleaning mileage '{value}': {e}\")\n",
    "            return np.nan\n",
    "    \n",
    "    df['Mileage'] = df['Mileage'].apply(clean_mileage)\n",
    "    \n",
    "    # Clean and standardize Price column\n",
    "    def clean_price(value):\n",
    "        if pd.isna(value) or (isinstance(value, str) and value.strip() == '') or value == 'ASK':\n",
    "            return np.nan\n",
    "        \n",
    "        try:\n",
    "            # Keep only digits and decimal points\n",
    "            value_str = str(value)\n",
    "            cleaned_value = re.sub(r'[^\\d.]', '', value_str)\n",
    "            if not cleaned_value:\n",
    "                return np.nan\n",
    "            return float(cleaned_value)\n",
    "        except (ValueError, TypeError) as e:\n",
    "            print(f\"Error cleaning price '{value}': {e}\")\n",
    "            return np.nan\n",
    "    \n",
    "    df['Price'] = df['Price'].apply(clean_price)\n",
    "    \n",
    "    # Clean and standardize fuel type with proper categorization\n",
    "    def standardize_fuel_type(value):\n",
    "        if pd.isna(value) or (isinstance(value, str) and value.strip() == ''):\n",
    "            return np.nan\n",
    "            \n",
    "        value_lower = str(value).lower().strip()\n",
    "        \n",
    "        # Standardize common fuel types\n",
    "        if 'petrol' in value_lower or 'petroleum' in value_lower:\n",
    "            if 'hybrid' in value_lower:\n",
    "                return 'petrol hybrid'\n",
    "            return 'petrol'\n",
    "        elif 'diesel' in value_lower:\n",
    "            if 'hybrid' in value_lower:\n",
    "                return 'diesel hybrid'\n",
    "            return 'diesel'\n",
    "        elif 'hybrid' in value_lower:\n",
    "            return 'hybrid'\n",
    "        elif 'electric' in value_lower:\n",
    "            return 'electric'\n",
    "        else:\n",
    "            return value_lower\n",
    "    \n",
    "    df['Fuel type'] = df['Fuel type'].apply(standardize_fuel_type)\n",
    "    \n",
    "    # Clean and standardize transmission\n",
    "    def standardize_transmission(value):\n",
    "        if pd.isna(value) or (isinstance(value, str) and value.strip() == ''):\n",
    "            return np.nan\n",
    "            \n",
    "        value_lower = str(value).lower().strip()\n",
    "        \n",
    "        # Map common transmission types\n",
    "        if value_lower in ['automatic', 'at']:\n",
    "            return 'automatic'\n",
    "        elif value_lower in ['manual', 'mt']:\n",
    "            return 'manual'\n",
    "        elif 'mt' in value_lower:  # For cases like '5MT', '6MT'\n",
    "            return 'manual'\n",
    "        elif 'automatic' in value_lower:\n",
    "            return 'automatic'\n",
    "        else:\n",
    "            return value_lower\n",
    "    \n",
    "    df['Transmission'] = df['Transmission'].apply(standardize_transmission)\n",
    "    \n",
    "    # Clean and standardize Drive type\n",
    "    def standardize_drive_type(value):\n",
    "        if pd.isna(value) or (isinstance(value, str) and value.strip() == ''):\n",
    "            return np.nan\n",
    "            \n",
    "        value_lower = str(value).lower().strip()\n",
    "        \n",
    "        # Standardize common drive types\n",
    "        if value_lower in ['awd', '4wd', '4x4']:\n",
    "            return 'awd'\n",
    "        elif value_lower in ['2wd', 'fwd', 'ff']:\n",
    "            return 'fwd'\n",
    "        elif value_lower in ['rwd', 'fr', 'rr']:\n",
    "            return 'rwd'\n",
    "        else:\n",
    "            return value_lower\n",
    "    \n",
    "    df['Drive type'] = df['Drive type'].apply(standardize_drive_type)\n",
    "    \n",
    "    # Clean and standardize Body type\n",
    "    def standardize_body_type(value):\n",
    "        if pd.isna(value) or (isinstance(value, str) and value.strip() == ''):\n",
    "            return np.nan\n",
    "            \n",
    "        value_lower = str(value).lower().strip()\n",
    "        \n",
    "        # Map common body types to standard categories\n",
    "        if 'suv' in value_lower:\n",
    "            return 'suv'\n",
    "        elif value_lower in ['saloon', 'sedan']:\n",
    "            return 'sedan'\n",
    "        elif 'hatchback' in value_lower:\n",
    "            return 'hatchback'\n",
    "        elif value_lower in ['wagon', 'estate']:\n",
    "            return 'wagon'\n",
    "        elif value_lower in ['minivan', 'van']:\n",
    "            return 'van'\n",
    "        elif 'pickup' in value_lower:\n",
    "            return 'pickup'\n",
    "        elif 'coupe' in value_lower:\n",
    "            return 'coupe'\n",
    "        else:\n",
    "            return value_lower\n",
    "    \n",
    "    df['Body type'] = df['Body type'].apply(standardize_body_type)\n",
    "    \n",
    "    # Clean Condition column - handle mixed data types\n",
    "    def clean_condition(value):\n",
    "        if pd.isna(value) or (isinstance(value, str) and value.strip() == ''):\n",
    "            return np.nan\n",
    "            \n",
    "        value_str = str(value).lower().strip()\n",
    "        \n",
    "        # Try to extract numeric rating\n",
    "        numeric_match = re.search(r'(\\d+(?:\\.\\d+)?)', value_str)\n",
    "        if numeric_match:\n",
    "            try:\n",
    "                return float(numeric_match.group(1))\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "                \n",
    "        # Map textual conditions to numeric scale\n",
    "        condition_map = {\n",
    "            'excellent': 5.0,\n",
    "            'very good': 4.5,\n",
    "            'good': 4.0,\n",
    "            'average': 3.0,\n",
    "            'below average': 2.0,\n",
    "            'poor': 1.0,\n",
    "            'new': 5.0,\n",
    "            'unsold': np.nan,  # Not condition-related\n",
    "            'sold': np.nan,     # Not condition-related\n",
    "            'foreign used': np.nan,  # Not condition-related\n",
    "            'locally used': np.nan,  # Not condition-related\n",
    "            'ready for import': np.nan  # Not condition-related\n",
    "        }\n",
    "        \n",
    "        # Try to match to our map\n",
    "        for key, rating in condition_map.items():\n",
    "            if key in value_str:\n",
    "                return rating\n",
    "                \n",
    "        # Failed to parse condition\n",
    "        return np.nan\n",
    "    \n",
    "    # Create a new column for numeric condition and keep original\n",
    "    df['Condition_Original'] = df['Condition']\n",
    "    df['Condition'] = df['Condition'].apply(clean_condition)\n",
    "    \n",
    "    # Clean Seats column to extract just the number\n",
    "    def clean_seats(value):\n",
    "        if pd.isna(value) or (isinstance(value, str) and value.strip() == ''):\n",
    "            return np.nan\n",
    "            \n",
    "        # Extract the first number from the string\n",
    "        match = re.search(r'(\\d+)', str(value))\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return np.nan\n",
    "    \n",
    "    df['Seats'] = df['Seats'].apply(clean_seats)\n",
    "    \n",
    "    # Clean other numerical columns with better error handling\n",
    "    def safe_numeric_clean(value):\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "        \n",
    "        if isinstance(value, (int, float)):\n",
    "            return float(value)\n",
    "            \n",
    "        try:\n",
    "            value_str = str(value).strip()\n",
    "            # Keep only digits and decimal points\n",
    "            cleaned = re.sub(r'[^\\d.]', '', value_str)\n",
    "            if not cleaned:\n",
    "                return np.nan\n",
    "            return float(cleaned)\n",
    "        except (ValueError, TypeError):\n",
    "            return np.nan\n",
    "    \n",
    "    # Apply to remaining numeric columns\n",
    "    for col in ['Horsepower', 'Torque', 'Acceleration']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(safe_numeric_clean)\n",
    "    \n",
    "    # =================== STEP 3: IDENTIFY AND HANDLE OUTLIERS ===================\n",
    "    print(\"\\n=== Checking for Outliers ===\")\n",
    "    \n",
    "    # Check for outliers in numerical columns\n",
    "    numeric_cols = ['Engine size', 'Mileage', 'Price', 'Horsepower', 'Torque', 'Acceleration']\n",
    "    numeric_cols = [col for col in numeric_cols if col in df.columns]\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if df[col].count() > 0:  # Skip empty columns\n",
    "            q1 = df[col].quantile(0.25)\n",
    "            q3 = df[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 3 * iqr  # More permissive lower bound (3*IQR)\n",
    "            upper_bound = q3 + 3 * iqr  # More permissive upper bound (3*IQR)\n",
    "            \n",
    "            # Flag outliers\n",
    "            outliers_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "            outlier_count = outliers_mask.sum()\n",
    "            \n",
    "            if outlier_count > 0:\n",
    "                print(f\"Found {outlier_count} outliers in {col} ({outlier_count/df[col].count()*100:.2f}%)\")\n",
    "                print(f\"  Range: [{df[col].min()}, {df[col].max()}], IQR bounds: [{lower_bound}, {upper_bound}]\")\n",
    "                \n",
    "                # Create reasonable caps based on column\n",
    "                if col == 'Engine size':\n",
    "                    # Cap engine size between 500cc and 10000cc\n",
    "                    df.loc[df[col] < 500, col] = np.nan\n",
    "                    df.loc[df[col] > 10000, col] = np.nan\n",
    "                elif col == 'Horsepower':\n",
    "                    # Cap horsepower at reasonable values (5-2000hp)\n",
    "                    df.loc[df[col] < 5, col] = np.nan\n",
    "                    df.loc[df[col] > 2000, col] = np.nan\n",
    "                elif col == 'Torque':\n",
    "                    # Cap torque at reasonable values (30-1500Nm)\n",
    "                    df.loc[df[col] < 30, col] = np.nan\n",
    "                    df.loc[df[col] > 1500, col] = np.nan\n",
    "                elif col == 'Acceleration':\n",
    "                    # Cap acceleration at reasonable values (1-30s for 0-100km/h)\n",
    "                    df.loc[df[col] < 1, col] = np.nan\n",
    "                    df.loc[df[col] > 30, col] = np.nan\n",
    "                elif col == 'Price':\n",
    "                    # Cap price at reasonable lower/upper bounds for car market\n",
    "                    df.loc[df[col] < 10000, col] = np.nan  # Likely error if below 10,000 (currency)\n",
    "                    \n",
    "                print(f\"  After capping: Range: [{df[col].min()}, {df[col].max()}]\")\n",
    "    \n",
    "    # =================== STEP 4: IMPUTE MISSING VALUES ===================\n",
    "    print(\"\\n=== Imputing Missing Values ===\")\n",
    "    \n",
    "    # First, identify rare Make-Model combinations to handle them separately\n",
    "    model_counts = df.groupby(['Make', 'Model']).size().reset_index(name='count')\n",
    "    rare_models = model_counts[model_counts['count'] < 5]\n",
    "    rare_model_pairs = set(map(tuple, rare_models[['Make', 'Model']].values))\n",
    "    \n",
    "    # Create a mask for rare models\n",
    "    def is_rare_model(row):\n",
    "        if pd.isna(row['Make']) or pd.isna(row['Model']):\n",
    "            return False\n",
    "        return (row['Make'], row['Model']) in rare_model_pairs\n",
    "    \n",
    "    rare_model_mask = df.apply(is_rare_model, axis=1)\n",
    "    print(f\"Found {rare_model_mask.sum()} instances of rare models (fewer than 5 entries)\")\n",
    "    \n",
    "    # Helper function to safely get mode of a series (handles empty series)\n",
    "    def safe_mode(series):\n",
    "        if series.empty or series.isna().all():\n",
    "            return np.nan\n",
    "        mode_values = series.mode()\n",
    "        return mode_values[0] if not mode_values.empty else np.nan\n",
    "    \n",
    "    # Helper function to safely get median of a series (handles empty series)\n",
    "    def safe_median(series):\n",
    "        if series.empty or series.isna().all():\n",
    "            return np.nan\n",
    "        return series.median()\n",
    "    \n",
    "    # Columns to impute\n",
    "    categorical_cols = ['Fuel type', 'Transmission', 'Drive type', 'Body type', 'Seats']\n",
    "    categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
    "    \n",
    "    numerical_cols = ['Engine size', 'Horsepower', 'Torque', 'Acceleration', 'Year']\n",
    "    numerical_cols = [col for col in numerical_cols if col in df.columns]\n",
    "    \n",
    "    # Smarter imputation strategy:\n",
    "    # 1. For non-rare models: Make-Model-Year → Make-Model → Make-Year → Make\n",
    "    # 2. For rare models: Make-Year → Make\n",
    "    \n",
    "    # Group by Make-Model-Year for non-rare models\n",
    "    for col in categorical_cols:\n",
    "        print(f\"Imputing {col}...\")\n",
    "        if col in df.columns and df[col].isna().any():\n",
    "            # For non-rare models\n",
    "            standard_mask = ~rare_model_mask & df[col].isna()\n",
    "            if standard_mask.any():\n",
    "                for make in df.loc[standard_mask, 'Make'].unique():\n",
    "                    for model in df.loc[(standard_mask) & (df['Make'] == make), 'Model'].unique():\n",
    "                        make_model_mask = (df['Make'] == make) & (df['Model'] == model) & standard_mask\n",
    "                        if make_model_mask.any():\n",
    "                            # First try Make-Model-Year\n",
    "                            for year in df.loc[make_model_mask, 'Year'].unique():\n",
    "                                year_mask = make_model_mask & (df['Year'] == year)\n",
    "                                if year_mask.any():\n",
    "                                    mode_value = safe_mode(df.loc[(df['Make'] == make) & \n",
    "                                                             (df['Model'] == model) & \n",
    "                                                             (df['Year'] == year) & \n",
    "                                                             ~df[col].isna(), col])\n",
    "                                    if not pd.isna(mode_value):\n",
    "                                        df.loc[year_mask, col] = mode_value\n",
    "            \n",
    "            # For remaining missing values in non-rare models, try Make-Model\n",
    "            still_missing = ~rare_model_mask & df[col].isna()\n",
    "            if still_missing.any():\n",
    "                for make in df.loc[still_missing, 'Make'].unique():\n",
    "                    for model in df.loc[(still_missing) & (df['Make'] == make), 'Model'].unique():\n",
    "                        make_model_mask = (df['Make'] == make) & (df['Model'] == model) & still_missing\n",
    "                        if make_model_mask.any():\n",
    "                            mode_value = safe_mode(df.loc[(df['Make'] == make) & \n",
    "                                                     (df['Model'] == model) & \n",
    "                                                     ~df[col].isna(), col])\n",
    "                            if not pd.isna(mode_value):\n",
    "                                df.loc[make_model_mask, col] = mode_value\n",
    "            \n",
    "            # For rare models, use Make only\n",
    "            rare_missing = rare_model_mask & df[col].isna()\n",
    "            if rare_missing.any():\n",
    "                for make in df.loc[rare_missing, 'Make'].unique():\n",
    "                    make_mask = (df['Make'] == make) & rare_missing\n",
    "                    if make_mask.any():\n",
    "                        mode_value = safe_mode(df.loc[(df['Make'] == make) & ~df[col].isna(), col])\n",
    "                        if not pd.isna(mode_value):\n",
    "                            df.loc[make_mask, col] = mode_value\n",
    "            \n",
    "            # Final fallback for all: global mode\n",
    "            still_missing = df[col].isna()\n",
    "            if still_missing.any():\n",
    "                mode_value = safe_mode(df.loc[~df[col].isna(), col])\n",
    "                if not pd.isna(mode_value):\n",
    "                    df.loc[still_missing, col] = mode_value\n",
    "            \n",
    "            print(f\"  - Remaining missing in {col}: {df[col].isna().sum()}\")\n",
    "    \n",
    "    # Similar approach for numerical columns but using median instead of mode\n",
    "    for col in numerical_cols:\n",
    "        print(f\"Imputing {col}...\")\n",
    "        if col in df.columns and df[col].isna().any():\n",
    "            # For non-rare models\n",
    "            standard_mask = ~rare_model_mask & df[col].isna()\n",
    "            if standard_mask.any():\n",
    "                for make in df.loc[standard_mask, 'Make'].unique():\n",
    "                    for model in df.loc[(standard_mask) & (df['Make'] == make), 'Model'].unique():\n",
    "                        make_model_mask = (df['Make'] == make) & (df['Model'] == model) & standard_mask\n",
    "                        if make_model_mask.any():\n",
    "                            # First try Make-Model-Year\n",
    "                            for year in df.loc[make_model_mask, 'Year'].unique():\n",
    "                                if pd.notna(year):  # Skip NaN years\n",
    "                                    year_mask = make_model_mask & (df['Year'] == year)\n",
    "                                    if year_mask.any():\n",
    "                                        median_value = safe_median(df.loc[(df['Make'] == make) & \n",
    "                                                               (df['Model'] == model) & \n",
    "                                                               (df['Year'] == year) & \n",
    "                                                               ~df[col].isna(), col])\n",
    "                                        if not pd.isna(median_value):\n",
    "                                            df.loc[year_mask, col] = median_value\n",
    "            \n",
    "            # For remaining missing values in non-rare models, try Make-Model\n",
    "            still_missing = ~rare_model_mask & df[col].isna()\n",
    "            if still_missing.any():\n",
    "                for make in df.loc[still_missing, 'Make'].unique():\n",
    "                    for model in df.loc[(still_missing) & (df['Make'] == make), 'Model'].unique():\n",
    "                        make_model_mask = (df['Make'] == make) & (df['Model'] == model) & still_missing\n",
    "                        if make_model_mask.any():\n",
    "                            median_value = safe_median(df.loc[(df['Make'] == make) & \n",
    "                                                     (df['Model'] == model) & \n",
    "                                                     ~df[col].isna(), col])\n",
    "                            if not pd.isna(median_value):\n",
    "                                df.loc[make_model_mask, col] = median_value\n",
    "            \n",
    "            # For rare models, use Make only \n",
    "            rare_missing = rare_model_mask & df[col].isna()\n",
    "            if rare_missing.any():\n",
    "                for make in df.loc[rare_missing, 'Make'].unique():\n",
    "                    make_mask = (df['Make'] == make) & rare_missing\n",
    "                    if make_mask.any():\n",
    "                        median_value = safe_median(df.loc[(df['Make'] == make) & ~df[col].isna(), col])\n",
    "                        if not pd.isna(median_value):\n",
    "                            df.loc[make_mask, col] = median_value\n",
    "            \n",
    "            # Final fallback for all: global median\n",
    "            still_missing = df[col].isna()\n",
    "            if still_missing.any():\n",
    "                median_value = safe_median(df.loc[~df[col].isna(), col])\n",
    "                if not pd.isna(median_value):\n",
    "                    df.loc[still_missing, col] = median_value\n",
    "                \n",
    "            print(f\"  - Remaining missing in {col}: {df[col].isna().sum()}\")\n",
    "    \n",
    "    # Special handling for Condition column\n",
    "    if 'Condition' in df.columns and df['Condition'].isna().any():\n",
    "        print(\"Imputing Condition...\")\n",
    "        # Calculate the global median once for efficiency\n",
    "        global_median = safe_median(df['Condition'].dropna())\n",
    "        \n",
    "        # Apply global median where missing\n",
    "        df.loc[df['Condition'].isna(), 'Condition'] = global_median\n",
    "        print(f\"  - Remaining missing in Condition: {df['Condition'].isna().sum()}\")\n",
    "    \n",
    "    # =================== STEP 5: ADD VALIDATION CHECKS ===================\n",
    "    print(\"\\n=== Running Validation Checks ===\")\n",
    "    \n",
    "    # Validation: Check that diesel engines are typically >1500cc\n",
    "    if 'Fuel type' in df.columns and 'Engine size' in df.columns:\n",
    "        diesel_mask = df['Fuel type'] == 'diesel'\n",
    "        diesel_engines = df.loc[diesel_mask & df['Engine size'].notna(), 'Engine size']\n",
    "        if not diesel_engines.empty:\n",
    "            min_diesel_size = diesel_engines.min()\n",
    "            if min_diesel_size < 1500:\n",
    "                print(f\"WARNING: Found diesel engines smaller than 1500cc (minimum: {min_diesel_size})\")\n",
    "                # Flagging potential errors for diesel engine sizes\n",
    "                suspicious_diesel = diesel_mask & (df['Engine size'] < 1500)\n",
    "                print(f\"  Found {suspicious_diesel.sum()} suspicious diesel engines < 1500cc\")\n",
    "            else:\n",
    "                print(f\"✅ All diesel engines are >1500cc (minimum: {min_diesel_size})\")\n",
    "    \n",
    "    # Validation: Check that automatic transmission is properly identified\n",
    "    if 'Transmission' in df.columns:\n",
    "        auto_variants = df['Transmission'].value_counts().get('automatic', 0)\n",
    "        print(f\"✅ Found {auto_variants} cars with automatic transmission\")\n",
    "    \n",
    "    # Validation: Check for valid price ranges\n",
    "    if 'Price' in df.columns:\n",
    "        price_stats = df['Price'].describe()\n",
    "        print(f\"Price range: {price_stats['min']:,.0f} to {price_stats['max']:,.0f}, \"\n",
    "              f\"Mean: {price_stats['mean']:,.0f}\")\n",
    "        \n",
    "        # Flag suspiciously low prices\n",
    "        low_price = df['Price'] < price_stats['25%'] / 2\n",
    "        if low_price.any():\n",
    "            print(f\"WARNING: Found {low_price.sum()} vehicles with suspiciously low prices\")\n",
    "    \n",
    "    # Check if all Makes are populated\n",
    "    if 'Make' in df.columns:\n",
    "        missing_makes = df['Make'].isna().sum()\n",
    "        if missing_makes > 0:\n",
    "            print(f\"WARNING: Still have {missing_makes} rows with missing Make\")\n",
    "        else:\n",
    "            print(\"✅ All rows have Make populated\")\n",
    "    \n",
    "    # Generate summary statistics for key columns after cleaning\n",
    "    print(\"\\n=== Data Summary After Cleaning ===\")\n",
    "    print(f\"Original rows: {original_shape[0]}\")\n",
    "    print(f\"Final rows: {len(df)}\")\n",
    "    \n",
    "    print(f\"\\nMissing values remaining:\")\n",
    "    for col in df.columns:\n",
    "        missing = df[col].isna().sum()\n",
    "        if missing > 0:\n",
    "            print(f\"  - {col}: {missing} ({missing/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # =================== STEP 6: SAVE THE CLEAN DATASET ===================\n",
    "    print(f\"\\nSaving cleaned dataset to: {output_path}\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Return the clean DataFrame for further use if needed\n",
    "    return df, output_path\n",
    "\n",
    "# Execute the cleaning if script is run directly\n",
    "if __name__ == \"__main__\":\n",
    "    # Update this path to your dataset location\n",
    "    file_path = r\"C:\\Users\\Ricky\\Desktop\\For Fun Projects\\SBT-Japan\\datasets\\merged_used_cars_cleaned.csv\"\n",
    "    \n",
    "    # Run the cleaning function\n",
    "    cleaned_df, output_path = clean_used_cars_dataset(file_path)\n",
    "    \n",
    "    # Print sample of cleaned data\n",
    "    print(\"\\n=== Sample of Cleaned Data ===\")\n",
    "    print(cleaned_df.head())\n",
    "    \n",
    "    # Sample of previously problematic rows (728-730)\n",
    "    if len(cleaned_df) > 730:\n",
    "        print(\"\\nCheck rows 728-730 after cleaning:\")\n",
    "        print(cleaned_df.iloc[728:731][['Make', 'Model']])\n",
    "    \n",
    "    print(f\"\\nCleaning complete! Dataset saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
